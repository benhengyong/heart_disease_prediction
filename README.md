### Overview and Purpose

### Data Analysis
Link to Tableau: https://public.tableau.com/app/profile/braydon.nugent/viz/Project4_Group2_Explatory_Visualisations/ExplatoryAnalysis?publish=yes

### Building ML Models

### Optimising Neural  Network Model
We set out to make our neural network super accurate by combining insights from Random Forest analysis and Tableau visualizations to figure out the best data columns to test.

First, we prepped our dataset, making sure all the data in our columns were integers and cutting out any unnecessary ones. Then, we tweaked the setup of our neural network, adjusting the hidden node layers to use "relu" and the outer layer to use "sigmoid".

To make sure our model was performing at its best, we ran it through a bunch of tests. We kept an eye on metrics like accuracy and loss, using "binary crossentropy" to measure loss. We used the "adam" optimizer to help train the model, and we played around with the number of epochs to fine-tune its performance.

This whole process was all about finding the sweet spot for our neural network, making sure it was as accurate as possible by tinkering with different settings and configurations.
### Discussion

### what is the challenge and how your group overcome it? What is the limitation of the data/model?
